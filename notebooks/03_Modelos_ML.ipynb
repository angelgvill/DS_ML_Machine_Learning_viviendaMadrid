{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inicio-notebook\"></a>\n",
    "# Proyecto End to End de Machine Learning \n",
    "### Viviendas en venta en Madrid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Librerías\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importación agrupada de librerías necesarias en este notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from PIL import Image\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, LabelBinarizer, MultiLabelBinarizer, OneHotEncoder \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# Añado el directorio padre (del que está este notebook) a sys.path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from scripts.utils_agv import ini_inspec, crear_tabla_resumen, categoricas, numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"baseline\"></a> \n",
    "## 17. ¿Qué modelos? Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(viene del notebook anterior, el conjunto de datos 'test' aquí NUNCA llegan los datos de test) He guardado los dataframe a probar en formato parquet.\n",
    "\n",
    "Primero, importamos el parquet que queremos probar en todos nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduzco aquí el Dataframe en cuestión \n",
    "file = 'train_sinOutliers_df5.parquet'\n",
    "file_path = f'../data/processed/{file}' \n",
    "df = pd.read_parquet(file_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1337, 25)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente hago una separación de entrenamiento y validación, para evaluar mis modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['priceLog'], axis=1)  # Eliminamos la variable objetivo y variables no útiles\n",
    "y = df['priceLog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: (1069, 24)\n",
      "Tamaño del conjunto de validación: (268, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tamaño del conjunto de validación: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (escala original): 597,080.46 €\n",
      "MAPE (escala original): 69.13 %\n"
     ]
    }
   ],
   "source": [
    "# Modelo Dummy (Baseline)\n",
    "dummy = DummyRegressor(strategy=\"mean\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en la escala logarítmica\n",
    "y_pred_dummy_log = dummy.predict(X_val)\n",
    "\n",
    "# Transformar los valores a la escala original\n",
    "y_val_original = np.expm1(y_val)  # Revertir log(price)\n",
    "y_pred_dummy_original = np.expm1(y_pred_dummy_log)\n",
    "\n",
    "# Evaluación con RMSE en la escala original\n",
    "rmse_dummy = np.sqrt(mean_squared_error(y_val_original, y_pred_dummy_original))\n",
    "print(f\"RMSE (escala original): {rmse_dummy:,.2f} €\")\n",
    "\n",
    "# Evaluación con MAPE en la escala original\n",
    "mape_dummy = mean_absolute_percentage_error(y_val_original, y_pred_dummy_original) * 100\n",
    "print(f\"MAPE (escala original): {mape_dummy:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
