{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inicio-notebook\"></a>\n",
    "# Proyecto End to End de Machine Learning \n",
    "### Viviendas en venta en Madrid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Librerías\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importación agrupada de librerías necesarias en este notebook\n",
    "import pandas as pd\n",
    "from pandas import StringDtype\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from PIL import Image\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, LabelBinarizer, MultiLabelBinarizer, OneHotEncoder \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# Añado el directorio padre (del que está este notebook) a sys.path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from scripts.utils_agv import ini_inspec, crear_tabla_resumen, categoricas, numericas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comprension-variables\"></a>\n",
    "## 4. Compresión de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga de los datos guardados en el anterior paso\n",
    "df = pd.read_csv('../data/processed/ide_viv_limpieza0_2025-03-11.csv', index_col='propertyCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name = 'ID'\n",
    "# Mover la columna 'price' a la primera posición\n",
    "columnas = ['price'] + [col for col in df.columns if col != 'price']\n",
    "df = df.reindex(columns=columnas)\n",
    "print (df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_inspec(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de esta primera inspección, confirmado que no hay duplicados, vamos a abordar los problemas que observo: valores faltantes, contenidos como diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento (desdoblado) de las columnas cuyos valores son diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['detailedType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['suggestedTexts'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a separar en columnas aquellas cuyos datos son dicionarios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIÓN PARA EXPANDIR CELDAS CON CONTENIDO DICCIONARIOS\n",
    "def expand_dict_columns(df):\n",
    "    \"\"\"\n",
    "    Expande las columnas del dataframe de Idealista que contienen diccionarios.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (pandas.DataFrame): DataFrame con datos de Idealista\n",
    "    \n",
    "    Retorna:\n",
    "    pandas.DataFrame: DataFrame con las columnas expandidas\n",
    "    \"\"\"\n",
    "    # Hacer una copia del dataframe original para no modificarlo\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    def parse_dict_safely(value):\n",
    "        \"\"\"Convierte strings a diccionarios de forma segura sin usar ast\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return {}\n",
    "        if isinstance(value, dict):\n",
    "            return value\n",
    "        if isinstance(value, str) and value.strip():\n",
    "            try:\n",
    "                # Intentar convertir usando json.loads\n",
    "                return json.loads(value)\n",
    "            except json.JSONDecodeError:\n",
    "                try:\n",
    "                    # Si falla, corregimos comillas simples a dobles\n",
    "                    value = value.replace(\"'\", \"\\\"\")\n",
    "                    return json.loads(value)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si aún falla, retornar vacío\n",
    "                    return {}\n",
    "        return {}\n",
    "    \n",
    "    def process_column(column_name, field_mappings):\n",
    "        \"\"\"\n",
    "        Procesa una columna de diccionario y extrae campos específicos.\n",
    "        \n",
    "        Parámetros:\n",
    "        column_name (str): Nombre de la columna a procesar\n",
    "        field_mappings (dict): Diccionario donde la clave es el nombre del campo \n",
    "                               a extraer y el valor es un valor por defecto\n",
    "        \"\"\"\n",
    "        if column_name not in df_processed.columns:\n",
    "            return\n",
    "            \n",
    "        # Convertir strings a diccionarios\n",
    "        df_processed[column_name] = df_processed[column_name].apply(parse_dict_safely)\n",
    "        \n",
    "        # Extraer cada campo del diccionario\n",
    "        for field, default_value in field_mappings.items():\n",
    "            new_column_name = f\"{column_name}_{field}\"\n",
    "            df_processed[new_column_name] = df_processed[column_name].apply(\n",
    "                lambda x: x.get(field, default_value) if isinstance(x, dict) else default_value\n",
    "            )\n",
    "    \n",
    "    # Definir los campos a extraer para cada columna\n",
    "    column_fields = {\n",
    "        'suggestedTexts': {'subtitle': None, 'title': None},\n",
    "        'detailedType': {'typology': None, 'subTypology': None},\n",
    "        'parkingSpace': {\n",
    "            'hasParkingSpace': False, \n",
    "            'isParkingSpaceIncludedInPrice': None,\n",
    "            'parkingSpacePrice': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Procesar cada columna\n",
    "    for column, fields in column_fields.items():\n",
    "        process_column(column, fields)\n",
    "    \n",
    "    # Eliminar las columnas originales\n",
    "    columns_to_drop = [col for col in column_fields.keys() if col in df_processed.columns]\n",
    "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA SALTADA intento de tratamiento de todas las columnas con diccionarios, incluso Parking\n",
    "pass\n",
    "def expand_dict_columns2(df):\n",
    "    \"\"\"\n",
    "    Expande las columnas del dataframe de Idealista que contienen diccionarios.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (pandas.DataFrame): DataFrame con datos de Idealista\n",
    "    \n",
    "    Retorna:\n",
    "    pandas.DataFrame: DataFrame con las columnas expandidas\n",
    "    \"\"\"\n",
    "    # Hacer una copia del dataframe original para no modificarlo\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    def parse_dict_safely(value):\n",
    "        \"\"\"Convierte strings a diccionarios de forma segura sin usar ast\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return {}\n",
    "        if isinstance(value, dict):\n",
    "            return value\n",
    "        if isinstance(value, str) and value.strip():\n",
    "            try:\n",
    "                # Intentar convertir usando json.loads\n",
    "                return json.loads(value)\n",
    "            except json.JSONDecodeError:\n",
    "                try:\n",
    "                    # Si falla, corregimos comillas simples a dobles\n",
    "                    value = value.replace(\"'\", \"\\\"\")\n",
    "                    return json.loads(value)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Si aún falla, retornar vacío\n",
    "                    return {}\n",
    "        return {}\n",
    "    \n",
    "    # Procesar columna suggestedTexts\n",
    "    if 'suggestedTexts' in df_processed.columns:\n",
    "        df_processed['suggestedTexts'] = df_processed['suggestedTexts'].apply(parse_dict_safely)\n",
    "        df_processed['suggestedTexts_subtitle'] = df_processed['suggestedTexts'].apply(\n",
    "            lambda x: x.get('subtitle') if isinstance(x, dict) else None\n",
    "        )\n",
    "        df_processed['suggestedTexts_title'] = df_processed['suggestedTexts'].apply(\n",
    "            lambda x: x.get('title') if isinstance(x, dict) else None\n",
    "        )\n",
    "    \n",
    "    # Procesar columna detailedType\n",
    "    if 'detailedType' in df_processed.columns:\n",
    "        df_processed['detailedType'] = df_processed['detailedType'].apply(parse_dict_safely)\n",
    "        df_processed['detailedType_typology'] = df_processed['detailedType'].apply(\n",
    "            lambda x: x.get('typology') if isinstance(x, dict) else None\n",
    "        )\n",
    "        df_processed['detailedType_subTypology'] = df_processed['detailedType'].apply(\n",
    "            lambda x: x.get('subTypology') if isinstance(x, dict) else None\n",
    "        )\n",
    "    \n",
    "    # Procesar columna parkingSpace - Esto lo dejamos explícito para manejar mejor los casos especiales\n",
    "    if 'parkingSpace' in df_processed.columns:\n",
    "        # Convertir strings a diccionarios y manejar valores NaN\n",
    "        df_processed['parkingSpace'] = df_processed['parkingSpace'].apply(parse_dict_safely)\n",
    "        \n",
    "        # Extraer hasParkingSpace - valor por defecto es False\n",
    "        df_processed['parkingSpace_hasParkingSpace'] = df_processed['parkingSpace'].apply(\n",
    "            lambda x: x.get('hasParkingSpace', False) if isinstance(x, dict) else False\n",
    "        )\n",
    "        \n",
    "        # Extraer isParkingSpaceIncludedInPrice - Sin valor por defecto para preservar NaN cuando no existe\n",
    "        df_processed['parkingSpace_isParkingSpaceIncludedInPrice'] = df_processed['parkingSpace'].apply(\n",
    "            lambda x: x.get('isParkingSpaceIncludedInPrice') if isinstance(x, dict) else pd.NA\n",
    "        )\n",
    "        \n",
    "        # Extraer parkingSpacePrice - Convertimos a float explícitamente si existe\n",
    "        df_processed['parkingSpace_parkingSpacePrice'] = df_processed['parkingSpace'].apply(\n",
    "            lambda x: float(x.get('parkingSpacePrice')) if isinstance(x, dict) and 'parkingSpacePrice' in x and x['parkingSpacePrice'] is not None else pd.NA\n",
    "        )\n",
    "    \n",
    "    # Eliminar las columnas originales que contenían diccionarios\n",
    "    columns_to_drop = []\n",
    "    for col in ['suggestedTexts', 'detailedType', 'parkingSpace']:\n",
    "        if col in df_processed.columns:\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ampliado_dict = expand_dict_columns(df, dict_columns=['otraColumnaDict', 'segundaColumnaDict'])\n",
    "df_ampliado_dict = expand_dict_columns(df)\n",
    "df_ampliado_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversión Parking Space en multiples columnas bien: finalmente esto no lo he realizado, aunque hice multiples intentos, y nunca lograba que me funcionara bien\n",
    "# TODO mejorar esto\n",
    "# df_ampliado_dict['parkingSpace_hasParkingSpace'].unique()\n",
    "# df_ampliado_dict['parkingSpace_isParkingSpaceIncludedInPrice'].unique()\n",
    "# df_ampliado_dict['parkingSpace_parkingSpacePrice'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA=df_ampliado_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA['detailedType_typology'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA['detailedType_subTypology'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación nueva variable IMPORTANTE: Terraza\n",
    "(correspondería en el esquema al punto 13.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pretende considerar el hecho de que un piso tenga o no terraza como un elemento de estudio en los datos. Sin embargo, este dato no se obtiene en el scrapping de Idealista. Así pues, se busca y sondea en la descripción del inmueble, con la seguridad de que si el piso tiene terraza, y salvo alguna excepción (terraza mínima, residual o trastero), esta estará citada en dicha descripción. \n",
    "Se asume cierto error en esta asunción, pero por contra, es seguro que mejorará las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar todas las filas donde 'terraza' aparece en Descripción\n",
    "pd.set_option('display.max_colwidth', None)  # No limitar ancho de columna\n",
    "pd.set_option('display.max_rows', None)      # Mostrar todas las filas\n",
    "print(len(dfA[dfA['description'].str.contains('terraza', case=False, na=False)]))\n",
    "dfA[dfA['description'].str.contains('terraza', case=False, na=False)][['propertyType', 'description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sondeada la columna description, incorporo la nueva columna con todas las cadenas de texto a buscar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear patrones de regex para buscar terrazas\n",
    "patrones_terraza = [\n",
    "    r', terraza,', \n",
    "    r'la terraza', \n",
    "    r'doble terraza',\n",
    "    r'balcón/ terraza', \n",
    "    r'balcón/terraza',\n",
    "    r'con terraza',\n",
    "    r'magnifica terraza',\n",
    "    r'amplia terraza',\n",
    "    r'gran terraza',\n",
    "    r'grandes terrazas',\n",
    "    r'terraza privada',\n",
    "    r'una terraza',\n",
    "    r'dos terrazas',\n",
    "    r'terraza de \\d+ metros',\n",
    "    r'terraza de \\d+ m2'\n",
    "]\n",
    "\n",
    "# Combinar todos los patrones en una sola expresión regular\n",
    "patron_combinado = '|'.join(patrones_terraza)\n",
    "\n",
    "# Función para detectar si hay mención de terraza según los patrones\n",
    "def tiene_terraza(texto):\n",
    "    if pd.isna(texto):\n",
    "        return 0\n",
    "    # Convertir a minúsculas para hacer la búsqueda insensible a mayúsculas\n",
    "    texto = texto.lower()\n",
    "    return 1 if re.search(patron_combinado, texto) else 0\n",
    "\n",
    "# Insertar la nueva columna después de 'price'\n",
    "# Primero obtenemos la posición de la columna 'price'\n",
    "posicion_price = dfA.columns.get_loc('price')\n",
    "\n",
    "# Creamos la serie con los valores de terraza\n",
    "serie_terraza = dfA['description'].apply(tiene_terraza)\n",
    "\n",
    "# Insertamos la columna después de 'price'\n",
    "# Primero creamos una copia del dataframe para no modificar el original\n",
    "dfA_Terraza = dfA.copy()\n",
    "\n",
    "# Insertamos la columna en la posición deseada\n",
    "cols = list(dfA_Terraza.columns)\n",
    "cols.insert(posicion_price + 1, 'terraza')\n",
    "dfA_Terraza = dfA_Terraza.reindex(columns=cols)\n",
    "dfA_Terraza['terraza'] = serie_terraza\n",
    "\n",
    "# Verificamos algunas filas para comprobar que funciona\n",
    "dfA_Terraza.head()[['price', 'terraza', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # No limitar ancho de columna\n",
    "pd.set_option('display.max_rows', None)      # Mostrar todas las filas\n",
    "dfA_Terraza[dfA_Terraza['description'].str.contains('terraza', case=False, na=False)][['propertyType', 'terraza', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA_Terraza.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfA_Terraza.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_inspec(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, una rápida analítica de cada una de las variables.\n",
    "1. **Variable**: nombre variable/alias\n",
    "2. **Data type**: cualitativa, cuantitativa, ordinal, continua...¿?\n",
    "3. **Segmento**: clasificar las variables según su significado. Si son variables demográficas, económicas, identificadores, tiempo...\n",
    "4. **Expectativas**: un pequeño indicador personal de si resultará útil la variable. ¿Necesito esta variable para la solución? ¿Cómo de importante será esta variable? ¿Esta info la recoge otra variable ya vista?\n",
    "5. **Conclusiones**: después del análisis anterior, llegar a unas conclusiones sobre la importancia de la variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable |Dtype |tipo |faltantes |segmento |expectativas |conclusiones|\n",
    "|--|--|--|--|--|--|--|\n",
    "|||||unidades |descripción |\n",
    "|propertyCode(ID)| int64| entero|código numérico Idealista||ID\n",
    "|numPhotos|int64 | entero||ud.|posible categorizador|probar| \n",
    "|floor |object | float(discreto)| 64|piso|valiosa|importante|\n",
    "|price | float64| continuo||€|predicción|target|\n",
    "|terraza | bool| booleano|0|indicador|nueva variable creada|previsiblemente importante|\n",
    "|propertyType| object |categórico|\n",
    "|size| float64| continuo|\n",
    "|exterior| object| booleano |24|bool|valioso|importante|\n",
    "|rooms| int64| entero|||||\n",
    "|bathrooms| int64| entero|\n",
    "|address| object| categórico|\n",
    "|district| object| categórico|\n",
    "|neighborhood| object| categórico|\n",
    "|latitude|float64 | continuo|\n",
    "|longitude|float64 | continuo|\n",
    "|description| object| categórico|\n",
    "|hasVideo| bool| booleano |\n",
    "|status| object| categórico|5|valoración incremental discreta|valiosa|ordinal encoder|\n",
    "|hasLift|  object| booleano |6| bool|valiosa|importante|\n",
    "|priceByArea| float64 | continuo|0|€/m2 |colinealidad| a eliminar|\n",
    "|hasPlan| bool| booleano |\n",
    "|has3DTour|bool| booleano |\n",
    "|has360| bool| booleano |\n",
    "|topPlus| bool| booleano |\n",
    "|suggestedTexts_subtitle|object| categórico| | ¿datos adicionales?|¿redundante?||\n",
    "|suggestedTexts_title |object| categórico| | ¿datos adicionales?|¿redundante?||\n",
    "|detailedType_typology |   object| categórico| | clasificación|¿redundante?|eliminar|\n",
    "|detailedType_subTypology | object| categórico|1524| clasificación|¿redundante?|eliminar|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al inicio de la sección](#comprension-variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reduccion-variables\"></a>\n",
    "## 5. Reducción (tratamiento) de variables preliminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso que estamos trabajando solo con el distrito centro, puedo eliminara la columna 'district'. En caso de trabajar con varios distritos la mantendría para introducirla en el módelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop('district', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de la columna 'floor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['floor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de valores faltantes en 'floor': {df2['floor'].isnull().sum()}\")\n",
    "print(\"Filas con valores faltantes en 'floor':\")\n",
    "df2[df2['floor'].isnull()].head().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar pandas para que no trunque el texto\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Filtrar el DataFrame para obtener las filas donde 'floor' tiene valores NaN\n",
    "df2[df2['floor'].isnull()][['floor','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con los IDs donde 'floor' es NaN\n",
    "ids_floor_nan = df2[df2['floor'].isnull()].index.tolist()\n",
    "\n",
    "# Guardar la lista de IDs en una variable para uso futuro\n",
    "print(f\"Lista de IDs con 'floor' como NaN (longitud {len(ids_floor_nan)}):\")\n",
    "print(ids_floor_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función de imputación se basa en busca piso en la descripción, asumiendo que en el distrito centro no existen muchos bloques de más de 5 alturas, y es esos casos excepcionales, es seguro que hubieran indicado explícitamente el piso en su campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputar_floor(dataframe):\n",
    "    \"\"\"\n",
    "    Imputa valores en la columna 'floor' basándose en palabras clave encontradas en la columna 'description'\n",
    "    para las filas donde 'floor' es NaN.\n",
    "\n",
    "    :param dataframe: DataFrame que debe contener las columnas 'floor' y 'description'.\n",
    "    :return: DataFrame con los valores imputados en 'floor'.\n",
    "    \"\"\"\n",
    "    # Diccionario que mapea palabras clave a valores de 'floor'\n",
    "    floor_mapping = {\n",
    "        1: ['1º', '1ª', 'primer piso', 'piso primero', 'planta primera', 'primera planta'],\n",
    "        2: ['2º', '2ª', 'segundo piso', 'piso segundo', 'planta segunda', 'segunda planta'],\n",
    "        3: ['3º', '3ª', 'tercer piso', 'piso tercero', 'tercera planta'],\n",
    "        4: ['4º', '4ª', 'cuarto piso', 'cuarta planta'],\n",
    "        5: ['5º', '5ª', 'quinto piso', 'quinta planta'],\n",
    "        6: ['6º', '6ª','sexto piso', 'sexta planta']\n",
    "    }\n",
    "\n",
    "    # Filtrar las filas donde 'floor' es NaN\n",
    "    filtro = dataframe[dataframe['floor'].isnull()]\n",
    "\n",
    "    # Iterar sobre el filtro para verificar palabras clave\n",
    "    for index, row in filtro.iterrows():\n",
    "        descripcion = str(row['description']).lower()  # Convertir a minúsculas\n",
    "        for floor, keywords in floor_mapping.items():\n",
    "            # Verificar si alguna palabra clave está en la descripción\n",
    "            if any(keyword in descripcion for keyword in keywords):\n",
    "                dataframe.at[index, 'floor'] = floor  # Asignar el valor correspondiente\n",
    "                break  # Romper el bucle después de encontrar una coincidencia\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputar_floor(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['floor'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['floor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el resultado de la lista de las filas imputadas\n",
    "df2.loc[ids_floor_nan, ['floor','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar el resto de NaNs restante a 0\n",
    "df2['floor'] = df2['floor'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de la columna 'exterior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['exterior'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "# Filtrar el DataFrame para obtener las filas donde 'exterior' tiene valores NaN\n",
    "df2[df2['exterior'].isnull()][['exterior','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con los IDs donde 'exterior' es NaN\n",
    "ids_exterior_nan = df2[df2['exterior'].isnull()].index.tolist()\n",
    "\n",
    "# Guardar la lista de IDs en una variable para uso futuro\n",
    "print(f\"Lista de IDs con 'exterior' como NaN (longitud {len(ids_exterior_nan)}):\")\n",
    "print(ids_exterior_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputar_exterior(dataframe):\n",
    "    \"\"\"\n",
    "    Esta función recorre un DataFrame y realiza imputaciones en la columna 'exterior' basándose\n",
    "    en las expresiones encontradas en la columna 'description'.\n",
    "    \n",
    "    - Si la descripción contiene expresiones positivas, reemplaza NaN en 'exterior' por True.\n",
    "    - Si la descripción contiene expresiones negativas, reemplaza NaN en 'exterior' por False.\n",
    "    - Si no se encuentra ninguna coincidencia, también reemplaza NaN por False.\n",
    "    \n",
    "    :param dataframe: DataFrame que debe contener las columnas 'exterior' y 'description'.\n",
    "    :return: DataFrame con los valores imputados en 'exterior'.\n",
    "    \"\"\"\n",
    "    # Listas de expresiones a buscar\n",
    "    expresiones_positivas = ['es exterior', 'exterior', 'piso exterior']\n",
    "    expresiones_negativas = ['interior', 'es interior', 'no es exterior']\n",
    "\n",
    "    # Filtrar las filas donde 'exterior' es NaN\n",
    "    filtro = dataframe[dataframe['exterior'].isnull()]\n",
    "\n",
    "    # Iterar sobre el filtro para verificar y reemplazar en el DataFrame original\n",
    "    for index, row in filtro.iterrows():\n",
    "        descripcion = str(row['description']).lower()\n",
    "        if any(neg in descripcion for neg in expresiones_negativas):\n",
    "            # Si hay una expresión negativa, reemplazar NaN por False\n",
    "            dataframe.at[index, 'exterior'] = False\n",
    "        elif any(pos in descripcion for pos in expresiones_positivas):\n",
    "            # Si hay una expresión positiva, reemplazar NaN por True\n",
    "            dataframe.at[index, 'exterior'] = True\n",
    "        else:\n",
    "            # Si no hay coincidencias, reemplazar NaN por False\n",
    "            dataframe.at[index, 'exterior'] = False\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputar_exterior (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la lista de las filas imputadas\n",
    "df2.loc[ids_exterior_nan, ['exterior']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['exterior'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de la columna 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con los IDs donde 'status' es NaN\n",
    "ids_status_nan = df2[df2['status'].isnull()].index.tolist()\n",
    "\n",
    "# Guardar la lista de IDs en una variable para uso futuro\n",
    "print(f\"Lista de IDs con 'status' como NaN (longitud {len(ids_status_nan)}):\")\n",
    "print(ids_status_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "# Filtrar el DataFrame para obtener las filas donde 'exterior' tiene valores NaN\n",
    "df2[df2['status'].isnull()][['status','description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se comprueba que quiza Idealista no permita indicar (o no se desea) Rehabilitación integral o Rehabilitación o Proyecto como un status válido, y aparece en la descripción. Se buscan dichas palabras, y en el caso de aparecer, se crean etiquetas que luego se convertirán en números. Si no se encuentra, se omite y se presupone la peor circunstancia, esto es 'Reformar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputar_status(dataframe):\n",
    "    \"\"\"\n",
    "    Imputa valores en la columna 'status' basándose en palabras clave encontradas en la columna 'description'\n",
    "    para las filas donde 'status' es NaN.\n",
    "\n",
    "    :param dataframe: DataFrame que debe contener las columnas 'status' y 'description'.\n",
    "    :return: DataFrame con los valores imputados en 'status'.\n",
    "    \"\"\"\n",
    "    # Diccionario que mapea palabras clave a valores de 'status'\n",
    "    status_mapping = {\n",
    "        'Nueva': ['obra nueva', 'proyecto'],\n",
    "        'Rehab': ['rehabilitado', 'rehabilitación']\n",
    "    }\n",
    "\n",
    "    # Filtrar las filas donde 'status' es NaN\n",
    "    filtro = dataframe[dataframe['status'].isnull()]\n",
    "\n",
    "    # Iterar sobre el filtro para verificar palabras clave\n",
    "    for index, row in filtro.iterrows():\n",
    "        descripcion = str(row['description']).lower()  # Convertir a minúsculas\n",
    "        encontrado = False  # Bandera para saber si se asignó un valor\n",
    "        for status, keywords in status_mapping.items():\n",
    "            # Verificar si alguna palabra clave está en la descripción\n",
    "            if any(keyword in descripcion for keyword in keywords):\n",
    "                dataframe.at[index, 'status'] = status  # Asignar el valor correspondiente\n",
    "                encontrado = True\n",
    "                break  # Romper el bucle después de encontrar una coincidencia\n",
    "        if not encontrado:\n",
    "            # Si no se encontró ninguna palabra clave, se asume como la situación mas desfavorable, asignar 'Reformar'\n",
    "            dataframe.at[index, 'status'] = 'Reformar'\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputar_status(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la lista de las filas imputadas\n",
    "df2.loc[ids_status_nan, ['status']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de la columna 'lift'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['hasLift'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con los IDs donde 'hasLift' es NaN\n",
    "ids_lift_nan = df2[df2['hasLift'].isnull()].index.tolist()\n",
    "\n",
    "# Guardar la lista de IDs en una variable para uso futuro\n",
    "print(f\"Lista de IDs con 'hasLift' como NaN (longitud {len(ids_lift_nan)}):\")\n",
    "print(ids_lift_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar pandas para que no trunque el texto\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Filtrar el DataFrame para obtener las filas donde 'lift' tiene valores NaN\n",
    "df2[df2['hasLift'].isnull()][['hasLift','floor','description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tratamiento parte de la base de que en caso de no indicar si tiene ascensor (NaNs) y no decirlo clara y explícitamente en la descripción, debemos entender que no tiene ascensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas donde 'hasLift' es NaN\n",
    "filtro = df2[df2['hasLift'].isnull()]\n",
    "\n",
    "# Listas de expresiones a buscar\n",
    "expresiones_positivas = ['tiene ascensor', 'dispone de ascensor', 'con ascensor']\n",
    "expresiones_negativas = ['no tiene ascensor', 'no dispone de ascensor', 'sin ascensor']\n",
    "\n",
    "# Iterar sobre el filtro para verificar y reemplazar en el DataFrame original\n",
    "for index, row in filtro.iterrows():\n",
    "    descripcion = str(row['description']).lower()\n",
    "    if any(neg in descripcion for neg in expresiones_negativas):\n",
    "        # Si hay una expresión negativa, reemplazar NaN por False\n",
    "        df2.at[index, 'hasLift'] = False\n",
    "    elif any(pos in descripcion for pos in expresiones_positivas):\n",
    "        # Si hay una expresión positiva, reemplazar NaN por True\n",
    "        df2.at[index, 'hasLift'] = True\n",
    "    else:\n",
    "        # Si no hay ninguna coincidencia, reemplazar NaN por False\n",
    "        df2.at[index, 'hasLift'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['hasLift'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizo una función para imputar ascensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputar_ascensor(dataframe):\n",
    "    \"\"\"\n",
    "    Esta función recorre un DataFrame y realiza imputaciones en la columna 'hasLift' basándose\n",
    "    en las expresiones encontradas en la columna 'description'.\n",
    "    \n",
    "    - Si la descripción contiene expresiones positivas, reemplaza NaN en 'hasLift' por True.\n",
    "    - Si la descripción contiene expresiones negativas, reemplaza NaN en 'hasLift' por False.\n",
    "    - Si no se encuentra ninguna coincidencia, también reemplaza NaN por False.\n",
    "    \n",
    "    :param dataframe: DataFrame que debe contener las columnas 'hasLift' y 'description'.\n",
    "    :return: DataFrame con los valores imputados en 'hasLift'.\n",
    "    \"\"\"\n",
    "    # Listas de expresiones a buscar\n",
    "    expresiones_positivas = ['tiene ascensor', 'dispone de ascensor', 'con ascensor']\n",
    "    expresiones_negativas = ['no tiene ascensor', 'no dispone de ascensor', 'sin ascensor']\n",
    "\n",
    "    # Filtrar las filas donde 'hasLift' es NaN\n",
    "    filtro = dataframe[dataframe['hasLift'].isnull()]\n",
    "\n",
    "    # Iterar sobre el filtro para verificar y reemplazar en el DataFrame original\n",
    "    for index, row in filtro.iterrows():\n",
    "        descripcion = str(row['description']).lower()\n",
    "        if any(neg in descripcion for neg in expresiones_negativas):\n",
    "            # Si hay una expresión negativa, reemplazar NaN por False\n",
    "            dataframe.at[index, 'hasLift'] = False\n",
    "        elif any(pos in descripcion for pos in expresiones_positivas):\n",
    "            # Si hay una expresión positiva, reemplazar NaN por True\n",
    "            dataframe.at[index, 'hasLift'] = True\n",
    "        else:\n",
    "            # Si no hay coincidencias, reemplazar NaN por False\n",
    "            dataframe.at[index, 'hasLift'] = False\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputar_ascensor(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la lista de las filas imputadas\n",
    "df2.loc[ids_lift_nan, ['hasLift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de la columna 'detailedType_subTypology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['detailedType_subTypology'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['propertyType', 'detailedType_typology', 'detailedType_subTypology']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiadas las tres columnas que detallan tipo de propiedad, imputar los nulos de subTypology tendría el mismo resultado que usar propertyType, así que para estos datos donde todos los valores de detailedType son 'flat', decido eliminar en este momento detailedType, ambas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(['detailedType_typology', 'detailedType_subTypology'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen (df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Eliminados todos los nulos, algo que funcionaría relativamente bien en nuevos datos, pasamos a realizar algunas relaciones y estudio de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones en el tipo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Primeramente forzar el tipo del dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forzar_data_type(dataframe):\n",
    "    \"\"\"\n",
    "    Fuerza los tipos de datos en las columnas de un DataFrame según un mapeo predefinido.\n",
    "    \n",
    "    Tipos:\n",
    "    - int: numPhotos, rooms, bathrooms\n",
    "    - float: price, size, latitude, longitude, priceByArea\n",
    "    - str: [address, description, suggestedTexts_subtitle, suggestedTexts_title\n",
    "    - pd.StringDtype: floor, propertyType district, neighborhood, status\n",
    "    - bool: exterior, hasVideo, hasLift, hasPlan, has3DTour, has360, topPlus\n",
    "    \n",
    "    :param dataframe: DataFrame que será modificado.\n",
    "    :return: DataFrame con los tipos de datos forzados.\n",
    "    \"\"\"\n",
    "    # Diccionario con tipo de dato como clave y lista de columnas como valor\n",
    "    new_types = {\n",
    "        int: ['numPhotos', 'rooms', 'bathrooms'],\n",
    "        float: ['price', 'size', 'latitude', 'longitude', 'priceByArea'],\n",
    "        str: ['address', 'description', 'suggestedTexts_subtitle', 'suggestedTexts_title'],\n",
    "        pd.StringDtype(): ['floor', 'propertyType','district', 'neighborhood', 'status'],   #es lo mismo que poner \"string:.....\"\n",
    "        bool: ['exterior', 'hasVideo', 'hasLift', 'hasPlan', 'has3DTour', 'has360', 'topPlus']\n",
    "    }\n",
    "    \n",
    "    # Iterar sobre el diccionario y aplicar el tipo de dato a las columnas especificadas\n",
    "    for data_type, columns in new_types.items():\n",
    "        for column in columns:\n",
    "            if column in dataframe.columns:\n",
    "                # Limpieza previa para evitar errores de conversión\n",
    "                if data_type == str:\n",
    "                    dataframe[column] = dataframe[column].fillna('').astype(str)\n",
    "                elif data_type == StringDtype():\n",
    "                    dataframe[column] = dataframe[column].fillna('').astype(pd.StringDtype())  \n",
    "                elif data_type in [int, float]:\n",
    "                    dataframe[column] = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "                elif data_type == bool:\n",
    "                    dataframe[column] = dataframe[column].astype(bool)\n",
    "    dataframe.info()\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forzar_data_type (df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo 24 variables. Sigo las posibles caminos para reducir las variables. Comprobado que no tengo (5.1.) columnas con missing, (5.2.) variables repetidas, ni columnas con altísima cardinalidad, (5.3.), identificadores o valores únicos, que no aportan nada, paso a la **selección de variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Selección de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a realizar una matriz de correlación para visualizar las relaciones entre ellas, previamente a la realización de pruebas matemáticas para intentar eliminar alguna de las columnas que puedan ser irrelevantes para mi target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación SOLO con las columnas numéricas\n",
    "columnas_numericas = df3.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Crear matriz de correlación solo con las columnas numéricas\n",
    "matriz_corr = df3[columnas_numericas].corr()\n",
    "\n",
    "# Ordenar las columnas en función de su correlación con 'price'\n",
    "matriz_corr_target = matriz_corr['price'].sort_values(ascending=False)\n",
    "\n",
    "# Visualizar como un heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matriz_corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Matriz de Correlación (variables numéricas)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Mostrar correlación específica de las variables con el target 'price'\n",
    "print(\"Correlación de cada variable con 'price':\\n\")\n",
    "print(matriz_corr_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para matriz de correlación\n",
    "def cramer(x, y):\n",
    "    \"\"\"Calcula el coeficiente de Cramer para dos variables categóricas.\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def correlacion(dataframe, booleanas=None, categoricas=None, mostrar_valores=True):\n",
    "    \"\"\"\n",
    "    Calcula y grafica la matriz de correlación de un DataFrame con opciones para incluir booleanos y categóricas.\n",
    "    \n",
    "    Parámetros:\n",
    "    - dataframe: DataFrame con los datos.\n",
    "    - booleanas: Si 'bool', convierte booleanos a dummies (0/1) para incluirlos en la correlación.\n",
    "    - categoricas: Si 'Cramer', calcula la correlación entre variables categóricas usando Cramer’s V.\n",
    "    - mostrar_valores: por defecto True, muestra los valores numéricos en el heatmap.\n",
    "    \n",
    "    Devuelve:\n",
    "    - La matriz de correlación calculada.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # Convertir booleanos a numéricos si 'booleanas' es 'bool'\n",
    "    if booleanas == 'bool':\n",
    "        bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "        df[bool_cols] = df[bool_cols].astype(int)  # Convertir a 0/1\n",
    "    \n",
    "    # Seleccionar solo columnas numéricas\n",
    "    columnas_numericas = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    matriz_corr = df[columnas_numericas].corr()\n",
    "    \n",
    "    # Si se pide incluir categóricas con Cramer’s V\n",
    "    if categoricas == 'Cramer':\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(cat_cols) > 0:\n",
    "            cramer_corr = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "        \n",
    "        for col1 in cat_cols:\n",
    "            for col2 in cat_cols:\n",
    "                if col1 == col2:\n",
    "                    cramer_corr.loc[col1, col2] = 1  # Autocorrelación\n",
    "                else:\n",
    "                    cramer_corr.loc[col1, col2] = cramer(df[col1], df[col2])\n",
    "        \n",
    "        cramer_corr = cramer_corr.astype(float)\n",
    "        matriz_corr = matriz_corr.combine_first(cramer_corr)  # Unir ambas matrices\n",
    "\n",
    "    # Ordenar las columnas en función de su correlación con 'price'\n",
    "    if 'price' in matriz_corr.columns:\n",
    "        matriz_corr_target = matriz_corr['price'].sort_values(ascending=False)\n",
    "        print(\"\\nCorrelación de cada variable con 'price':\\n\")\n",
    "        print(matriz_corr_target)\n",
    "    \n",
    "    # Graficar heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(matriz_corr, annot=mostrar_valores, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Matriz de Correlación', fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    return matriz_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacion (df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrrelación limitada a price, priceByArea y size\n",
    "# Seleccionar las columnas de interés\n",
    "# cols = ['price', 'priceByArea', 'size']\n",
    "\n",
    "# # Calcular la matriz de correlación\n",
    "# corr_matrix = df3[cols].corr()\n",
    "\n",
    "# # Visualizar la matriz de correlación con un heatmap\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "# plt.title(\"Matriz de Correlación entre Price, PriceByArea y Size\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien fruto de la anterior matriz de correlación, procedería eliminar size como la que menos información aporta y alta correlación con price, parece bastante trampa, pues es un dato que se obtiene teniendo el precio de partida. Como esto no sucederá en datos futuros ni en test, y no debe suceder en validation, voy a eliminarla en este momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop('priceByArea', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversiones numéricas básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversión numérica mediante mapeo de 'floor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para encontrar detalle del significado de 'st' en 'floor' en la columna descripción\n",
    "# Ajustar pandas para que no trunque el texto\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Filtrar las filas donde 'floor' es igual a 'st'\n",
    "df3[df3['floor'] == 'st'][['floor', 'description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_floor_flotante(dataframe):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'floor' a valores numéricos según un mapeo predefinido.\n",
    "    \n",
    "    - Si el valor ya es numérico, se mantiene.\n",
    "    - Si es un texto reconocido ('bj', 'ss', etc.), se mapea a un número.\n",
    "    - Si es un número en texto ('3', '-1', etc.), se convierte a número.\n",
    "    - Si es un valor desconocido o vacío, se asigna 0.\n",
    "\n",
    "    :param dataframe: DataFrame con la columna 'floor'.\n",
    "    :return: DataFrame con 'floor' en formato numérico.\n",
    "    \"\"\"\n",
    "    # Diccionario de mapeo\n",
    "    floor_mapping = {\n",
    "        'entreplanta': 0.5, 'ent': 0.5, 'en': 0.5,\n",
    "        'baja': 0, 'bajo': 0, 'bj': 0, 'street': 0, 'st': 0,\n",
    "        'semisótano': -0.5, 'semisotano': -0.5, 'ss': -0.5,\n",
    "        'sótano': -1, 'sotano': -1, 'sot': -1\n",
    "    }\n",
    "\n",
    "    def map_floor(value):\n",
    "        # Si ya es numérico, se mantiene\n",
    "        if isinstance(value, (int, float)):\n",
    "            return value\n",
    "\n",
    "        # Si el valor es NaN o None, asignar 0\n",
    "        if pd.isna(value):\n",
    "            return 0\n",
    "\n",
    "        # Convertir a string y limpiar espacios\n",
    "        value = str(value).strip().lower()\n",
    "\n",
    "        # Si está en el diccionario, aplicar el mapeo\n",
    "        if value in floor_mapping:\n",
    "            return floor_mapping[value]\n",
    "\n",
    "        # Intentar convertir números en string ('3', '-1', etc.)\n",
    "        try:\n",
    "            return int(value) if value.isdigit() or value.lstrip('-').isdigit() else float(value)\n",
    "        except ValueError:\n",
    "            return 0  # Valores desconocidos se asignan a 0\n",
    "\n",
    "    # Aplicar la función y forzar a numérico\n",
    "    dataframe['floor'] = dataframe['floor'].apply(map_floor)\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num = convertir_floor_flotante(df3_num)\n",
    "df3_num.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num['floor'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversión numérica mediante mapeo de 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapear_status(dataframe):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'status' a valores numéricos según un mapeo predefinido.\n",
    "    \n",
    "    - 'Nueva' y 'newdevelopment' → 2\n",
    "    - 'good' → 1\n",
    "    - 'renew' → 0\n",
    "    - Cualquier otro valor (incluyendo NaN) → 0\n",
    "\n",
    "    :param dataframe: DataFrame con la columna 'status'.\n",
    "    :return: DataFrame con 'status' en formato numérico.\n",
    "    \"\"\"\n",
    "    # Diccionario de mapeo\n",
    "    status_mapping = {\n",
    "        'nueva': 2, 'newdevelopment': 2,\n",
    "        'good': 1,\n",
    "        'renew': 0\n",
    "    }\n",
    "\n",
    "    def map_status(value):\n",
    "        # Si ya es numérico, se mantiene\n",
    "        if isinstance(value, (int, float)):\n",
    "            return value\n",
    "\n",
    "        # Si el valor es NaN o None, asignar 0\n",
    "        if pd.isna(value):\n",
    "            return 0\n",
    "\n",
    "        # Convertir a string y limpiar espacios\n",
    "        value = str(value).strip().lower()\n",
    "\n",
    "        # Si está en el diccionario, aplicar el mapeo\n",
    "        return status_mapping.get(value, 0)\n",
    "\n",
    "    # Aplicar la función y forzar a numérico\n",
    "    dataframe['status'] = dataframe['status'].apply(map_status)\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num= mapear_status (df3_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num['status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversión automática resto de columnas binarias y (algunas) categóricas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num['propertyType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_a_numerico (dataframe):\n",
    "    \"\"\"\n",
    "    Convierte los valores del DataFrame:\n",
    "    - Convierte columnas booleanas a 0 y 1.\n",
    "    - Renombra 'district' → 'distrito' y 'neighborhood' → 'barrio'.\n",
    "    - Usa one-hot encoding para 'district' y 'neighborhood', si existen en el DataFrame.\n",
    "    - Renombra 'propertyType' → 'tipo' y usa one-hot encoding para 'tipo'.\n",
    "\n",
    "    :param dataframe: DataFrame de entrada.\n",
    "    :return: DataFrame transformado.\n",
    "    \"\"\"\n",
    "    df_numerico = dataframe.copy()\n",
    "\n",
    "    # Convertir booleanos a 0 y 1\n",
    "    bool_cols = ['exterior', 'hasVideo', 'hasLift', 'hasPlan', 'has3DTour', 'has360', 'topPlus']\n",
    "    for col in bool_cols:\n",
    "        if col in df_numerico.columns:\n",
    "            df_numerico[col] = df_numerico[col].astype(int)\n",
    "            \n",
    "    # Renombrar columnas antes de cualquier otra transformación\n",
    "    df_numerico = df_numerico.rename(columns={'district': 'distrito', 'neighborhood': 'barrio' , 'propertyType': 'tipo'})\n",
    "\n",
    "    # Aplicar one-hot encoding a 'district' y 'neighborhood' si existen\n",
    "    for col in ['districto', 'barrio']:\n",
    "        if col in df_numerico.columns:\n",
    "            dummies = pd.get_dummies(df_numerico[col], prefix=col, dtype=int)\n",
    "            df_numerico = pd.concat([df_numerico.drop(columns=[col]), dummies], axis=1)\n",
    "\n",
    "    # Aplicar one-hot encoding a 'tipo' para las categorías especificadas\n",
    "    if 'tipo' in df_numerico.columns:\n",
    "        tipos = ['flat', 'studio', 'penthouse', 'duplex']\n",
    "        # Verificar que las categorías estén en la columna 'tipo'\n",
    "        for tipo in tipos:\n",
    "            if tipo in df_numerico['tipo'].values:\n",
    "                # Crear columnas one-hot para cada tipo\n",
    "                df_numerico[f'tipo_{tipo}'] = (df_numerico['tipo'] == tipo).astype(int)\n",
    "        # Borrar la columna original 'tipo' después del one-hot encoding\n",
    "        # df_numerico = df_numerico.drop(columns=['tipo']) #no la borro, para el estudio de outliers. La borraré después\n",
    "        \n",
    "    return df_numerico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la conversión\n",
    "df3_num = convertir_a_numerico(df3_num)\n",
    "df3_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas (df3_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen (df3_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con los datos, función para convertir en csv y guardarlo.\n",
    "today =  date.today ()\n",
    "file_path = f'../data/processed/ide_viv_numerico0_{today}.csv' \n",
    "\n",
    "def df_to_csv(df):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"⚠️ El archivo '{file_path}' ya existe. No se sobrescribirá.\")\n",
    "    else:\n",
    "        df.to_csv(file_path)   #lo guarda en un csv con indice en propertyCode\n",
    "        print(f\"✅ Archivo guardado correctamente como '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guarda los datos pre separación Train-test en un csv con nombre establecido.\n",
    "df_to_csv(df3_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacion (df3_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "[Ir al principio de la sección 5](#reduccion-variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analisis-univariante\"></a>\n",
    "## 6. Análisis univariante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen (df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para gráficos univariante \n",
    "\n",
    "def graficos_uni(dataframe):\n",
    "    \"\"\"\n",
    "    Genera gráficos de distribución:\n",
    "    - Barras para variables categóricas, discretas y booleanas.\n",
    "    - Histogramas para variables continuas.\n",
    "\n",
    "    Parámetro:\n",
    "    - dataframe: DataFrame con los datos.\n",
    "    \"\"\"\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Excluir columnas no deseadas\n",
    "    excluidas = {'address', 'description', 'suggestedTexts_title'}\n",
    "    df = df.drop(columns=[col for col in excluidas if col in df.columns])\n",
    "\n",
    "    # Clasificar variables\n",
    "    categ_discretas = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categ_discretas += [col for col in df.select_dtypes(include=['int64']).columns if df[col].nunique() <= 15]\n",
    "    bool_vars = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "    continuas = [col for col in df.select_dtypes(include=['float64', 'int64']).columns if col not in categ_discretas]\n",
    "\n",
    "    num_vars = len(categ_discretas) + len(bool_vars) + len(continuas)\n",
    "    filas = -(-num_vars // 2)  # Redondeo hacia arriba\n",
    "\n",
    "    fig, axes = plt.subplots(filas, 2, figsize=(15, filas * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    i = 0\n",
    "    for col in categ_discretas:\n",
    "        sns.countplot(data=df, x=col, hue=col, ax=axes[i], legend=False,palette= 'light:gray')\n",
    "        axes[i].set_title(f\"Distribución de {col}\")\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        i += 1\n",
    "\n",
    "    for col in bool_vars:\n",
    "        sns.countplot(data=df, x=col, hue=col, ax=axes[i], palette={False: 'lightcoral', True: 'lightgreen'}, legend=False)\n",
    "        axes[i].set_title(f\"Distribución de {col}\")\n",
    "        i += 1\n",
    "\n",
    "    for col in continuas:\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i], bins=30, color=\"gray\", edgecolor=\"black\")\n",
    "        sns.kdeplot(df[col], ax=axes[i], color=\"red\", linewidth=2)\n",
    "        axes[i].set_title(f\"Distribución de {col}\")\n",
    "        i += 1\n",
    "\n",
    "    for j in range(i, len(axes)):  # Ocultar ejes vacíos\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficos_uni (df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ir al inicio de la sección 6](#analisis-univariante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analisis-bivariante\"></a>\n",
    "## 7. Análisis bivariante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de tener solo las variables numéricas, excluidas categóricas\n",
    "df3_pair = df3.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Usamos pairplot con el target 'price' para diferenciar por colores\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.pairplot(df3_pair, diag_kind='hist')\n",
    "#sns.pairplot(df3_pair, kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))  # Gráfico más grande para evitar achatamiento\n",
    "\n",
    "# Boxplot con mejor ajuste de tamaño\n",
    "sns.boxplot(data=df3, x='propertyType', y='price', hue='propertyType', palette='coolwarm', width=0.6)\n",
    "\n",
    "# Ajustes estéticos\n",
    "plt.title('Distribución de precios según tipo de vivienda', fontsize=16)\n",
    "plt.xlabel('Tipo de vivienda', fontsize=14)\n",
    "plt.ylabel('Precio (€)', fontsize=14)\n",
    "plt.xticks(rotation=45)  # Rotar etiquetas si es necesario\n",
    "\n",
    "# Evitar notación científica en el eje Y\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Agregar una cuadrícula horizontal para mejor referencia\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Guardar imagen\n",
    "plt.savefig('graf/boxplot_vivienda_precio.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['price'] > 6_000_000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3.index != 106035767]\n",
    "df3_num = df3_num[df3_num.index != 106035767]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))  # Gráfico más grande para evitar achatamiento\n",
    "\n",
    "# Boxplot con mejor ajuste de tamaño\n",
    "sns.boxplot(data=df3, x='propertyType', y='price', hue='propertyType', palette='coolwarm', width=0.6)\n",
    "\n",
    "# Ajustes estéticos\n",
    "plt.title('Distribución de precios según tipo de vivienda', fontsize=16)\n",
    "plt.xlabel('Tipo de vivienda', fontsize=14)\n",
    "plt.ylabel('Precio (€)', fontsize=14)\n",
    "plt.xticks(rotation=45)  # Rotar etiquetas si es necesario\n",
    "\n",
    "# Evitar notación científica en el eje Y\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Agregar una cuadrícula horizontal para mejor referencia\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Guardar imagen\n",
    "plt.savefig('graf/boxplot_vivienda_precio.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ir al inicio de la sección 7](#analisis-bivariante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eliminacion-variables\"></a>  \n",
    "## 8. Eliminación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen(df3_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a limpiar nuestros datos, después de analizados. Eliminaremos aquellas columnas de texto largo, descripción, dirección y título. También aquellas con alta cardinalidad, latitud y longitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[['suggestedTexts_subtitle', 'neighborhood']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobado que ambas columnas contienen los mismos datos, para mis efectos, eliminaré también la columna suggestedTexts_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_variables(dataframe):\n",
    "    \"\"\"\n",
    "    Elimina las columnas de texto largo (descripcion, direccion, etc.) y columnas con alta cardinalidad \n",
    "    como latitud y longitud.\n",
    "    \n",
    "    :param dataframe: DataFrame de entrada.\n",
    "    :return: DataFrame sin las columnas no deseadas.\n",
    "    \"\"\"\n",
    "    df_limpio = dataframe.copy()\n",
    "    \n",
    "    # Eliminar columnas de texto largo\n",
    "    columnas_texto_largo = ['description', 'address', 'suggestedTexts_title' ,'suggestedTexts_subtitle']\n",
    "    df_limpio = df_limpio.drop(columns=[col for col in columnas_texto_largo if col in df_limpio.columns])\n",
    "    \n",
    "    # Eliminar columnas con alta cardinalidad (latitud y longitud, o más de 100 valores únicos)\n",
    "    columnas_alta_cardinalidad = ['latitude', 'longitude']\n",
    "    \n",
    "    # También podemos eliminar aquellas columnas con más de 100 valores únicos (como un criterio genérico de alta cardinalidad)\n",
    "    for col in df_limpio.select_dtypes(include=['object']).columns:\n",
    "        if df_limpio[col].nunique() > 100:  # Umbral de cardinalidad alta\n",
    "            columnas_alta_cardinalidad.append(col)\n",
    "\n",
    "    # Eliminar las columnas con alta cardinalidad\n",
    "    df_limpio = df_limpio.drop(columns=[col for col in columnas_alta_cardinalidad if col in df_limpio.columns])\n",
    "\n",
    "    return df_limpio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = eliminar_variables(df3_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen (df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podrían haber duplicados que, bajo diferentes índices, sean el mismo anuncio, bien porque haya varios anunciantes para el mismo piso, o bien porque el mismo anunciante duplique su anuncio publicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar todas las filas duplicadas agrupadas\n",
    "duplicados = df4[df4.duplicated(keep=False)]  # keep=False para marcar todas las duplicadas, no solo las subsecuentes\n",
    "\n",
    "# Ordenar las filas duplicadas para visualizarlas juntas\n",
    "duplicados = duplicados.sort_values(by=list(df4.columns))  # Ordenar por todas las columnas\n",
    "\n",
    "duplicados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.drop_duplicates(keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Anomalías y errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crear_tabla_resumen(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas (df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se observan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"outliers\"></a> \n",
    "## 12. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya elimine uno clarísimo en el paso de análisis bivariante, ahora voy a ver todos los que están menos claros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el percentil 95 para cada tipo de propiedad\n",
    "percentile_95 = df4.groupby('tipo')['price'].quantile(0.95)\n",
    "\n",
    "# Mostrar los valores del percentil 95 por tipo de propiedad\n",
    "print(percentile_95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planteo un sistema de eliminar datos fuera de un porcentaje de datos de todos los totales: esto tiene el inconveniente de que no considera la distribución específica de mis datos, y que en distribuciones sesgadas, eliminará datos válidos. Además, trata todos los conjuntos de datos por igual. Eso sí, es simple, directo y predecible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el percentil como variable (por ejemplo, 95% = 0.95)\n",
    "percentil = 0.95\n",
    "\n",
    "# Calcular el percentil para cada tipo de propiedad\n",
    "percentil_valores = df4.groupby('tipo')['price'].quantile(percentil)\n",
    "\n",
    "# Mostrar los percentiles calculados\n",
    "print(f\"Percentiles al {percentil*100}%:\")\n",
    "print(percentil_valores)\n",
    "\n",
    "# Calcular el total de datos por tipo de propiedad\n",
    "total_datos_por_tipo = df4.groupby('tipo').size()\n",
    "\n",
    "# Calcular el número de datos que están por debajo del percentil para cada tipo de propiedad\n",
    "datos_percentil = df4[df4['price'] <= df4['tipo'].map(percentil_valores)].groupby('tipo').size()\n",
    "\n",
    "# Calcular los datos eliminados (total - datos dentro del percentil)\n",
    "datos_eliminados = total_datos_por_tipo - datos_percentil\n",
    "\n",
    "# Imprimir la información solicitada\n",
    "for prop_type in df4['tipo'].unique():\n",
    "    print(f\"\\nTipo de vivienda: {prop_type}\")\n",
    "    print(f\"Precio percentil {percentil*100}%: {percentil_valores[prop_type]:,.2f}€\")\n",
    "    print(f\"Número total de datos iniciales: {total_datos_por_tipo[prop_type]}\")\n",
    "    print(f\"Número total de datos en el percentil {percentil*100}%: {datos_percentil[prop_type]}\")\n",
    "    print(f\"Número de datos eliminados: {datos_eliminados[prop_type]}\")\n",
    "\n",
    "# Crear el gráfico de boxplot\n",
    "plt.figure(figsize=(16, 8))  # Gráfico más grande para evitar achatamiento\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(data=df4, x='tipo', y='price', hue= 'tipo', palette='coolwarm', width=0.6)\n",
    "\n",
    "# Agregar las líneas horizontales de los percentiles para cada tipo de propiedad\n",
    "for i, prop_type in enumerate(df4['tipo'].unique()):\n",
    "    color = plt.cm.coolwarm(i / len(df4['tipo'].unique()))  # Tomar un color de la paleta\n",
    "    plt.axhline(percentil_valores[prop_type], color=color, linestyle='--', \n",
    "                label=f'{prop_type} {percentil*100}%')\n",
    "    \n",
    "# Ajustes estéticos\n",
    "plt.title(f'Distribución de precios según tipo de vivienda (Percentil {percentil*100}%)', fontsize=16)\n",
    "plt.xlabel('Tipo de vivienda', fontsize=14)\n",
    "plt.ylabel('Precio (€)', fontsize=14)\n",
    "plt.xticks(rotation=45)  # Rotar etiquetas si es necesario\n",
    "\n",
    "# Evitar notación científica en el eje Y\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Agregar una cuadrícula horizontal para mejor referencia\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Leyenda\n",
    "plt.legend()\n",
    "\n",
    "# Guardar imagen\n",
    "plt.savefig(f'graf/boxplot_vivienda_precio_percentil{percentil*100}.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar por los extremos del boxplot es más adaptado, y es más riguroso. Una vez visto el gráfico anterior, prefiero este sistema, preferible en contextos de análisis inmobiliarios, para evitar distorsionar el modelo con propiedades de lujo o de valor bajo por condiciones singulares (usufructos, expropiaciones, subastas, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de outliers por criterio de bigotes.\n",
    "# Definir el factor de IQR como variable\n",
    "factor_iqr = 2  # Puedes cambiar este valor según lo necesites\n",
    "\n",
    "# Calcular los límites de los bigotes del boxplot para cada tipo de propiedad\n",
    "iqr_limits = {}\n",
    "\n",
    "for prop_type in df4['tipo'].unique():\n",
    "    q1 = np.percentile(df4[df4['tipo'] == prop_type]['price'], 25)\n",
    "    q3 = np.percentile(df4[df4['tipo'] == prop_type]['price'], 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - factor_iqr * iqr\n",
    "    upper_bound = q3 + factor_iqr * iqr\n",
    "    iqr_limits[prop_type] = (lower_bound, upper_bound)\n",
    "\n",
    "# Imprimir el resumen de datos\n",
    "for prop_type in df4['tipo'].unique():\n",
    "    lower_bound, upper_bound = iqr_limits[prop_type]\n",
    "    total_datos_inicial = len(df4[df4['tipo'] == prop_type])\n",
    "    total_datos_filtrados = len(df4[(df4['tipo'] == prop_type) & \n",
    "                                    (df4['price'] >= lower_bound) & \n",
    "                                    (df4['price'] <= upper_bound)])\n",
    "    datos_eliminados = total_datos_inicial - total_datos_filtrados\n",
    "\n",
    "    print(f\"Tipo: {prop_type}, \"\n",
    "          f\"Límite inferior: {lower_bound}, \"\n",
    "          f\"Límite superior: {upper_bound}, \"\n",
    "          f\"Número total de datos inicial: {total_datos_inicial}, \"\n",
    "          f\"Número total de datos dentro de los bigotes: {total_datos_filtrados}, \"\n",
    "          f\"Número de datos eliminados: {datos_eliminados}\")\n",
    "\n",
    "# Crear el gráfico de boxplot\n",
    "plt.figure(figsize=(16, 8))  # Gráfico más grande para evitar achatamiento\n",
    "\n",
    "# Dibujar el boxplot\n",
    "sns.boxplot(data=df4, x='tipo', y='price', hue='tipo', palette='coolwarm', width=0.6)\n",
    "\n",
    "# Agregar las líneas horizontales de los bigotes para cada tipo de propiedad\n",
    "for i, prop_type in enumerate(df4['tipo'].unique()):\n",
    "    lower_bound, upper_bound = iqr_limits[prop_type]\n",
    "    color = plt.cm.coolwarm(i / len(df4['tipo'].unique()))  # Tomar un color de la paleta\n",
    "    plt.axhline(upper_bound, color=color, linestyle='--', label=f'{prop_type} límite superior')\n",
    "    plt.axhline(lower_bound, color=color, linestyle=':', label=f'{prop_type} límite inferior')\n",
    "\n",
    "# Ajustes estéticos\n",
    "plt.title(f'Distribución de precios según tipo de vivienda (Bigotes {factor_iqr}×IQR)', fontsize=16)\n",
    "plt.xlabel('Tipo de vivienda', fontsize=14)\n",
    "plt.ylabel('Precio (€)', fontsize=14)\n",
    "plt.xticks(rotation=45)  # Rotar etiquetas si es necesario\n",
    "\n",
    "# Evitar notación científica en el eje Y\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Agregar una cuadrícula horizontal para mejor referencia\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Leyenda\n",
    "plt.legend()\n",
    "\n",
    "# Guardar imagen\n",
    "plt.savefig(f'graf/boxplot_vivienda_precio_IQR{factor_iqr}.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a automatizar la eliminación de outliers mediante una función, donde puedo elegir el método y el valor de corte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función fitrado outliers \n",
    "def eliminar_outliers(df, metodo=None, bigotes=1.5, percentil=0.95):\n",
    "    \"\"\"\n",
    "    Filtra outliers de un DataFrame según el método seleccionado.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame de entrada (se copia para no modificar el original).\n",
    "    - metodo (str): 'bigotes' para usar el método IQR o 'percentil' para usar percentiles.\n",
    "    - bigotes (float, opcional): Factor multiplicador de IQR para definir los bigotes (default = 1.5).\n",
    "    - percentil (float, opcional): Percentil para el umbral de eliminación (default = 0.95).\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: Nuevo DataFrame sin los outliers según el método elegido.\n",
    "    \"\"\"\n",
    "    # Copia del DataFrame original\n",
    "    df_filtrado = df.copy()\n",
    "\n",
    "    # Verificar si la columna 'tipo' existe\n",
    "    if 'tipo' not in df.columns or 'price' not in df.columns:\n",
    "        print(\"Error: El DataFrame debe contener las columnas 'tipo' y 'price'.\")\n",
    "        return None\n",
    "\n",
    "    # Si no se ha pasado ningún método, pedirlo\n",
    "    if metodo not in ['bigotes', 'percentil']:\n",
    "        print(\"Debe especificar un método de filtrado: 'bigotes' o 'percentil'.\")\n",
    "        return None\n",
    "\n",
    "    if metodo == 'bigotes':\n",
    "        print(f\"Filtrando con método 'bigotes' (IQR * {bigotes})...\")\n",
    "\n",
    "        # Calcular los límites de los bigotes para cada tipo de propiedad\n",
    "        for prop_type in df_filtrado['tipo'].unique():\n",
    "            subset = df_filtrado[df_filtrado['tipo'] == prop_type]['price']\n",
    "            q1, q3 = np.percentile(subset, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - bigotes * iqr\n",
    "            upper_bound = q3 + bigotes * iqr\n",
    "\n",
    "            # Filtrar el DataFrame\n",
    "            df_filtrado = df_filtrado[\n",
    "                ~((df_filtrado['tipo'] == prop_type) & \n",
    "                  ((df_filtrado['price'] < lower_bound) | (df_filtrado['price'] > upper_bound)))\n",
    "            ]\n",
    "\n",
    "    elif metodo == 'percentil':\n",
    "        print(f\"Filtrando con método 'percentil' ({percentil*100}%)...\")\n",
    "\n",
    "        # Calcular el percentil seleccionado para cada tipo de propiedad\n",
    "        percentiles = df_filtrado.groupby('tipo')['price'].quantile(percentil)\n",
    "\n",
    "        # Filtrar el DataFrame\n",
    "        df_filtrado = df_filtrado[\n",
    "            df_filtrado.apply(lambda row: row['price'] <= percentiles[row['tipo']], axis=1)\n",
    "        ]\n",
    "\n",
    "    # Mostrar resumen de filtrado\n",
    "    for prop_type in df['tipo'].unique():\n",
    "        total_datos_inicial = len(df[df['tipo'] == prop_type])\n",
    "        total_datos_filtrados = len(df_filtrado[df_filtrado['tipo'] == prop_type])\n",
    "        datos_eliminados = total_datos_inicial - total_datos_filtrados\n",
    "\n",
    "        print(f\"Tipo: {prop_type}, \"\n",
    "              f\"Número total de datos inicial: {total_datos_inicial}, \"\n",
    "              f\"Número total de datos filtrados: {total_datos_filtrados}, \"\n",
    "              f\"Número de datos eliminados: {datos_eliminados}\")\n",
    "\n",
    "    return df_filtrado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.shape)\n",
    "df4.columns.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = eliminar_outliers(df4, metodo='bigotes', bigotes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al inicio de la sección 12 ](#outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat-engi\"></a>\n",
    "<a href=\"#inicio-notebook\"><p style=\"text-align:right;\" href=\"#inicio-notebook\">Volver al índice</p></a> \n",
    "## 13. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación de la transformación logarítmica (probada en el notebook anterior) a la target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_log = df4.copy()  # Crear una copia del DataFrame original\n",
    "df4_log['price'] = np.log(df4_log['price'])  # Aplicar la transformación logarítmica solo en la copia\n",
    "df4_log.rename(columns={'price': 'priceLog'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df5_log = df5.copy()  # Crear una copia del DataFrame original\n",
    "df5_log['price'] = np.log(df5_log['price'])  # Aplicar la transformación logarítmica solo en la copia\n",
    "df5_log.rename(columns={'price': 'priceLog'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_log.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la columna 'tipo' que mantuvimos para clasificar outliers por grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminar la columna tipo en ambos dataframe df4, df5 a guardar para ML\n",
    "if 'tipo' in df4.columns:\n",
    "    df4_log.drop('tipo', axis=1, inplace=True, errors = 'ignore')\n",
    "else:\n",
    "    print(\"⚠️ Advertencia: La columna 'tipo' YA no existe en df4.\")\n",
    "\n",
    "if 'tipo' in df5.columns:\n",
    "    df5_log.drop('tipo', axis=1, inplace=True, errors = 'ignore')\n",
    "else:\n",
    "    print(\"⚠️ Advertencia: La columna 'tipo' YA no existe en df5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos nombrado archivo.\n",
    "today =  date.today ()\n",
    "file_path = f'../data/processed/ide_viv_numerico1_{today}.csv' \n",
    "\n",
    "#Guardo df4 como dataframe solo numérico para ML sin reducción de outliers\n",
    "df_to_csv (df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos nombrado archivo.\n",
    "today =  date.today ()\n",
    "file_path = f'../data/processed/ide_viv_numerico2_{today}.csv' \n",
    "\n",
    "#Guardo df4 como dataframe solo numérico para ML eliminados outliers\n",
    "df_to_csv (df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Reducción de dimensionalidad (asistidos)\n",
    "(basada en procesos automáticos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último probamos antes de pasar a la división de train y validation, feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los datos para la selección de características\n",
    "X = df5_log.drop(['priceLog'], axis=1)  # Eliminamos la variable objetivo y variables no útiles\n",
    "y = df5_log['priceLog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 1: Correlación con la variable objetivo\n",
    "correlaciones = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "print(\"\\nCorrelación de las características con precio:\")\n",
    "print(correlaciones.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 2: SelectKBest con f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_selector = selector.fit_transform(X, y)\n",
    "mask = selector.get_support()\n",
    "caracteristicas_seleccionadas = X.columns[mask]\n",
    "\n",
    "print(\"\\nCaracterísticas seleccionadas con SelectKBest:\")\n",
    "print(caracteristicas_seleccionadas.tolist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 3: Importancia de características con Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "importancias = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de características con Random Forest:\")\n",
    "print(importancias.head(10))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "importancias.head(10).plot(kind='barh')\n",
    "plt.title('Top 10 Características más Importantes (Random Forest)')\n",
    "plt.xlabel('Importancia')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 4: RFE con modelo de Random Forest\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Selección de características con RFE\n",
    "selector = RFE(model, n_features_to_select=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "# Obtener la importancia de las características seleccionadas\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "# Ordenar de mayor a menor\n",
    "importances = importances.sort_values(ascending=False)\n",
    "\n",
    "# Gráfico de barras con las 10 más importantes\n",
    "plt.figure(figsize=(12, 6))\n",
    "importances.head(10).plot(kind='barh', color='royalblue', edgecolor='black')\n",
    "plt.title('Top 10 Características más Importantes (Random Forest)')\n",
    "plt.xlabel('Importancia')\n",
    "plt.gca().invert_yaxis()  # Invertir el eje Y para mostrar la más importante arriba\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar las características seleccionadas\n",
    "print(\"Características seleccionadas por RFE:\")\n",
    "print(selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al inicio del Notebook](#inicio-notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. División en train y test\n",
    "(resevar una porción de los datos obtenidos para probar nuestros modelos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('price', axis=1),\n",
    "                                                    df['price'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________\n",
    "## XX. Transformaciones a aplicar a test (nuevos datos que vengan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrupación del código a aplicar a test, que apliqué en train\n",
    "\n",
    "# Detener la ejecución de esta celda\n",
    "\n",
    "# Todo lo que esté aquí no se ejecutará\n",
    "print(\"Esta celda está reservada para aplicar a los datos de test.\")\n",
    "return\n",
    "\n",
    "# # Definir la columna target\n",
    "target = 'price'  # Reemplaza con el nombre real de la columna target\n",
    "\n",
    "# Verificar si la columna target está en el DataFrame y eliminarla si existe\n",
    "if target in datos_test.columns:\n",
    "    datos_test = datos_test.drop(columns=[target])\n",
    "\n",
    "# Continuar con el resto del código sin interrupciones\n",
    "# df.drop_duplicates(keep='first', inplace=True)    #no es necesario eliminar duplicados en el test\n",
    "\n",
    "\n",
    "\n",
    "df = datos_test.reset_index(drop=True).set_index(\"propertyCode\")\n",
    "df.index.name = 'ID'\n",
    "\n",
    "col_eliminar = ['thumbnail','externalReference', 'priceInfo', 'operation', 'province', 'municipality',\n",
    "       'country', 'showAddress', 'url', 'newDevelopment', 'change', 'highlight', 'savedAd', \n",
    "       'notes','hasStaging', 'topNewDevelopment', 'parkingSpace' ,'newDevelopmentFinished', 'priceByArea']\n",
    "df.drop(col_eliminar, axis=1, inplace=True)\n",
    "\n",
    "df = expand_dict_columns(df)\n",
    "\n",
    "imputar_ascensor(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df4.drop('tipo', axis=1, inplace=True, errors = 'ignore')\n",
    "df5.drop('tipo', axis=1, inplace=True, errors = 'ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
